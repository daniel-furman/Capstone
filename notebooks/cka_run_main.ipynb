{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am_Ubq6xTaBz"
      },
      "source": [
        "# Contrastive Knowledge Assesment (CKA) Notebook for Running Experiments\n",
        "This notebook enables interactive experimentation with CKA for models including `Flan-ul2`, `Flan-t5`, `OPT`, `GPT-Neo`, `Roberta`, `Bert`, and `GPT2` models.\n",
        "The goal is to probe if factual statements are predicted at a higher probability than a given counterfactuals.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/daniel-furman/Capstone/blob/main/notebooks/cka_run_main.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uccv2X7WeJGv"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4gro-sOZz-O",
        "outputId": "95262ad7-07bf-432b-c5ec-ddbafe04bebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Capstone'...\n",
            "remote: Enumerating objects: 502, done.\u001b[K\n",
            "remote: Counting objects: 100% (159/159), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 502 (delta 74), reused 127 (delta 46), pack-reused 343\u001b[K\n",
            "Receiving objects: 100% (502/502), 24.32 MiB | 16.01 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/daniel-furman/Capstone.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaXMJaroH_TI",
        "outputId": "e80b6655-bbec-43aa-e5fa-a9710bf9c5dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.22.4 in /usr/local/lib/python3.9/dist-packages (from -r /content/Capstone/requirements.txt (line 1)) (1.22.4)\n",
            "Collecting sentencepiece==0.1.97\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (from -r /content/Capstone/requirements.txt (line 3)) (1.13.1+cu116)\n",
            "Collecting transformers==4.26.1\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.16.0\n",
            "  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.37.0\n",
            "  Downloading bitsandbytes-0.37.0-py3-none-any.whl (76.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.3/76.3 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from -r /content/Capstone/requirements.txt (line 7)) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->-r /content/Capstone/requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/Capstone/requirements.txt (line 4)) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/Capstone/requirements.txt (line 4)) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/Capstone/requirements.txt (line 4)) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/Capstone/requirements.txt (line 4)) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r /content/Capstone/requirements.txt (line 4)) (2022.6.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate==0.16.0->-r /content/Capstone/requirements.txt (line 5)) (5.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1->-r /content/Capstone/requirements.txt (line 4)) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1->-r /content/Capstone/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1->-r /content/Capstone/requirements.txt (line 4)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1->-r /content/Capstone/requirements.txt (line 4)) (4.0.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, bitsandbytes, huggingface-hub, accelerate, transformers\n",
            "Successfully installed accelerate-0.16.0 bitsandbytes-0.37.0 huggingface-hub-0.13.1 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/Capstone/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yjnEaRtKd8L"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJQImaEDTRMr"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jQTcgbNk-iA"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/Capstone/src/cka_scripts')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6lqNVBmhW-g"
      },
      "source": [
        "## CLI usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hic1I2GZD5BF",
        "outputId": "2c621f04-3628-432c-ec7b-115f1ada36e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for distilgpt2\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 762/762 [00:00<00:00, 121kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:01<00:00, 772kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:01<00:00, 410kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.23MB/s]\n",
            "Downloading pytorch_model.bin: 100% 353M/353M [00:00<00:00, 392MB/s]\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 46.9kB/s]\n",
            "Running comparisons...\n",
            "100% 21919/21919 [09:56<00:00, 36.75it/s]\n",
            "Done\n",
            "\n",
            "\n",
            "Score dict summary:\n",
            "{'distilgpt2': 'This model predicted 15099/21919 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.6568 while the mean p_true was 0.0095'}\n"
          ]
        }
      ],
      "source": [
        "!python run_cka.py configs.rome_full.gpt2_rome_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D4NDgkf8oAO",
        "outputId": "bd901cf9-2af8-418f-e17a-8a10261647b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for facebook/opt-125m\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 685/685 [00:00<00:00, 107kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 651/651 [00:00<00:00, 104kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 810kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 509kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 441/441 [00:00<00:00, 165kB/s]\n",
            "Downloading pytorch_model.bin: 100% 251M/251M [00:02<00:00, 87.2MB/s]\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "Downloading (…)neration_config.json: 100% 137/137 [00:00<00:00, 24.0kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:01<00:00,  1.54it/s]\n",
            "Done\n",
            "\n",
            "CKA for facebook/opt-350m\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 685/685 [00:00<00:00, 119kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 644/644 [00:00<00:00, 237kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 804kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:01<00:00, 407kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 441/441 [00:00<00:00, 166kB/s]\n",
            "Downloading pytorch_model.bin: 100% 663M/663M [00:06<00:00, 95.1MB/s]\n",
            "Downloading (…)neration_config.json: 100% 137/137 [00:00<00:00, 39.2kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:02<00:00,  1.03it/s]\n",
            "Done\n",
            "\n",
            "CKA for facebook/opt-1.3b\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 685/685 [00:00<00:00, 240kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 653/653 [00:00<00:00, 226kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 811kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 513kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 441/441 [00:00<00:00, 141kB/s]\n",
            "Downloading pytorch_model.bin: 100% 2.63G/2.63G [00:25<00:00, 104MB/s]\n",
            "Downloading (…)neration_config.json: 100% 137/137 [00:00<00:00, 48.2kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:02<00:00,  1.01it/s]\n",
            "Done\n",
            "\n",
            "\n",
            "Score dict summary:\n",
            "{'facebook/opt-125m': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9027 while the mean p_true was 0.2054', 'facebook/opt-350m': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9192 while the mean p_true was 0.4133', 'facebook/opt-1.3b': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9914 while the mean p_true was 0.4042'}\n"
          ]
        }
      ],
      "source": [
        "!python run_cka.py configs.tests.opt_v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qB6RpDTZsJC",
        "outputId": "df94d809-c6c9-4b8e-b7fd-a7cd50c1e600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for distilgpt2\n",
            "Loading  model...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  3.83it/s]\n",
            "Done\n",
            "\n",
            "CKA for gpt2\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 665/665 [00:00<00:00, 122kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:01<00:00, 939kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 512kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.22MB/s]\n",
            "Downloading pytorch_model.bin: 100% 548M/548M [00:01<00:00, 447MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 37.5kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  7.63it/s]\n",
            "Done\n",
            "\n",
            "CKA for gpt2-medium\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 718/718 [00:00<00:00, 264kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:01<00:00, 786kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 517kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.21MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.52G/1.52G [00:14<00:00, 101MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 40.0kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  4.34it/s]\n",
            "Done\n",
            "\n",
            "CKA for gpt2-large\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 666/666 [00:00<00:00, 244kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:01<00:00, 927kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 511kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.01MB/s]\n",
            "Downloading pytorch_model.bin: 100% 3.25G/3.25G [00:35<00:00, 91.4MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 44.7kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  3.05it/s]\n",
            "Done\n",
            "\n",
            "CKA for gpt2-xl\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 689/689 [00:00<00:00, 248kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:01<00:00, 940kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 513kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.00MB/s]\n",
            "Downloading pytorch_model.bin: 100% 6.43G/6.43G [00:16<00:00, 399MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 42.8kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:01<00:00,  2.34it/s]\n",
            "Done\n",
            "\n",
            "\n",
            "Score dict summary:\n",
            "{'distilgpt2': 'This model predicted 8/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.7032 while the mean p_true was 0.0085', 'gpt2': 'This model predicted 7/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.7577 while the mean p_true was 0.0887', 'gpt2-medium': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9267 while the mean p_true was 0.3705', 'gpt2-large': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9253 while the mean p_true was 0.431', 'gpt2-xl': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9854 while the mean p_true was 0.3997'}\n"
          ]
        }
      ],
      "source": [
        "!python run_cka.py configs.tests.gpt2_v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvAcClqXEtgB",
        "outputId": "866d7d7e-e9bd-4bbe-f0c1-04878a9854e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for google/bert_uncased_L-2_H-128_A-2\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 382/382 [00:00<00:00, 59.1kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 349kB/s]\n",
            "Downloading pytorch_model.bin: 100% 17.7M/17.7M [00:00<00:00, 101MB/s]\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  4.51it/s]\n",
            "Done\n",
            "\n",
            "CKA for google/bert_uncased_L-4_H-256_A-4\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 383/383 [00:00<00:00, 68.7kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 348kB/s]\n",
            "Downloading pytorch_model.bin: 100% 45.1M/45.1M [00:00<00:00, 93.3MB/s]\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00, 32.83it/s]\n",
            "Done\n",
            "\n",
            "CKA for google/bert_uncased_L-4_H-512_A-8\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 383/383 [00:00<00:00, 144kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 347kB/s]\n",
            "Downloading pytorch_model.bin: 100% 116M/116M [00:01<00:00, 62.6MB/s]\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-4_H-512_A-8 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00, 32.65it/s]\n",
            "Done\n",
            "\n",
            "CKA for google/bert_uncased_L-8_H-512_A-8\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 383/383 [00:00<00:00, 131kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 349kB/s]\n",
            "Downloading pytorch_model.bin: 100% 167M/167M [00:01<00:00, 103MB/s]\n",
            "Some weights of the model checkpoint at google/bert_uncased_L-8_H-512_A-8 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00, 18.28it/s]\n",
            "Done\n",
            "\n",
            "CKA for distilbert-base-uncased\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 9.64kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 181kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 348kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 524kB/s]\n",
            "Downloading pytorch_model.bin: 100% 268M/268M [00:02<00:00, 91.9MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00, 24.32it/s]\n",
            "Done\n",
            "\n",
            "CKA for bert-base-uncased\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 9.85kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 216kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 348kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 527kB/s]\n",
            "Downloading pytorch_model.bin: 100% 440M/440M [00:01<00:00, 369MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00, 13.49it/s]\n",
            "Done\n",
            "\n",
            "CKA for bert-large-uncased\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 9.83kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 571/571 [00:00<00:00, 216kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 349kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 525kB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.34G/1.34G [00:13<00:00, 101MB/s]\n",
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  7.06it/s]\n",
            "Done\n",
            "\n",
            "\n",
            "Score dict summary:\n",
            "{'google/bert_uncased_l-2_h-128_a-2': 'This model predicted 7/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.7106 while the mean p_true was 0.0016', 'google/bert_uncased_l-4_h-256_a-4': 'This model predicted 5/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.5592 while the mean p_true was 0.0069', 'google/bert_uncased_l-4_h-512_a-8': 'This model predicted 6/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.647 while the mean p_true was 0.0171', 'google/bert_uncased_l-8_h-512_a-8': 'This model predicted 8/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.7794 while the mean p_true was 0.0529', 'distilbert-base-uncased': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.927 while the mean p_true was 0.1193', 'bert-base-uncased': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9644 while the mean p_true was 0.3385', 'bert-large-uncased': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9549 while the mean p_true was 0.1689'}\n"
          ]
        }
      ],
      "source": [
        "!python run_cka.py configs.tests.bert_v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_AfmHruPIFv",
        "outputId": "918ab954-f273-40e5-c543-ddbf45bb7e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for distilroberta-base\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 480/480 [00:00<00:00, 74.7kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 811kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 508kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.21MB/s]\n",
            "Downloading pytorch_model.bin: 100% 331M/331M [00:03<00:00, 93.6MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  4.14it/s]\n",
            "Done\n",
            "\n",
            "CKA for roberta-base\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 90.6kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 804kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 514kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.02MB/s]\n",
            "Downloading pytorch_model.bin: 100% 501M/501M [00:01<00:00, 430MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00, 12.78it/s]\n",
            "Done\n",
            "\n",
            "CKA for xlm-roberta-base\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 615/615 [00:00<00:00, 30.0kB/s]\n",
            "Downloading (…)tencepiece.bpe.model: 100% 5.07M/5.07M [00:01<00:00, 3.20MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 9.10M/9.10M [00:01<00:00, 4.55MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.12G/1.12G [00:02<00:00, 482MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  9.31it/s]\n",
            "Done\n",
            "\n",
            "CKA for roberta-large\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 482/482 [00:00<00:00, 181kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 810kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 512kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.02MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.43G/1.43G [00:13<00:00, 104MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  6.86it/s]\n",
            "Done\n",
            "\n",
            "CKA for xlm-roberta-large\n",
            "Loading  model...\n",
            "Downloading (…)lve/main/config.json: 100% 616/616 [00:00<00:00, 219kB/s]\n",
            "Downloading (…)tencepiece.bpe.model: 100% 5.07M/5.07M [00:01<00:00, 3.22MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 9.10M/9.10M [00:02<00:00, 4.51MB/s]\n",
            "Downloading pytorch_model.bin: 100% 2.24G/2.24G [00:23<00:00, 94.9MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  6.14it/s]\n",
            "Done\n",
            "\n",
            "\n",
            "Score dict summary:\n",
            "{'distilroberta-base': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9352 while the mean p_true was 0.4082', 'roberta-base': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9583 while the mean p_true was 0.6529', 'xlm-roberta-base': 'This model predicted 7/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.7561 while the mean p_true was 0.1543', 'roberta-large': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9399 while the mean p_true was 0.6604', 'xlm-roberta-large': 'This model predicted 7/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.8092 while the mean p_true was 0.5288'}\n"
          ]
        }
      ],
      "source": [
        "!python run_cka.py configs.tests.roberta_v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WFNxz3elvq3",
        "outputId": "29062f1b-c592-4c3b-88c1-2abdc78b04dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for google/flan-t5-small\n",
            "Loading  model...\n",
            "Downloading spiece.model: 100% 792k/792k [00:00<00:00, 35.4MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 368kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 2.54k/2.54k [00:00<00:00, 1.03MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.40k/1.40k [00:00<00:00, 262kB/s]\n",
            "Downloading pytorch_model.bin: 100% 308M/308M [00:00<00:00, 462MB/s]\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "Downloading (…)neration_config.json: 100% 147/147 [00:00<00:00, 57.8kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:02<00:00,  1.31it/s]\n",
            "Done\n",
            "\n",
            "CKA for google/flan-t5-base\n",
            "Loading  model...\n",
            "Downloading spiece.model: 100% 792k/792k [00:00<00:00, 48.1MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 785kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 2.54k/2.54k [00:00<00:00, 916kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.40k/1.40k [00:00<00:00, 494kB/s]\n",
            "Downloading pytorch_model.bin: 100% 990M/990M [00:09<00:00, 104MB/s]\n",
            "Downloading (…)neration_config.json: 100% 147/147 [00:00<00:00, 54.9kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:02<00:00,  1.19it/s]\n",
            "Done\n",
            "\n",
            "CKA for google/flan-t5-large\n",
            "Loading  model...\n",
            "Downloading spiece.model: 100% 792k/792k [00:00<00:00, 85.2MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 773kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 2.54k/2.54k [00:00<00:00, 877kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 662/662 [00:00<00:00, 269kB/s]\n",
            "Downloading pytorch_model.bin: 100% 3.13G/3.13G [00:32<00:00, 95.7MB/s]\n",
            "Downloading (…)neration_config.json: 100% 147/147 [00:00<00:00, 51.5kB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:05<00:00,  1.67s/it]\n",
            "Done\n",
            "\n",
            "\n",
            "Score dict summary:\n",
            "{'google/flan-t5-small': 'This model predicted 8/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.855 while the mean p_true was 0.0204', 'google/flan-t5-base': 'This model predicted 8/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.8782 while the mean p_true was 0.2159', 'google/flan-t5-large': 'This model predicted 7/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.8183 while the mean p_true was 0.3736'}\n"
          ]
        }
      ],
      "source": [
        "!python run_cka.py configs.tests.flan_t5_v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZJS1RQElwMt",
        "outputId": "d1ab9b6c-43b5-4850-85d7-bc6be167bbed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for EleutherAI/gpt-neo-125M\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 560/560 [00:00<00:00, 86.6kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.01k/1.01k [00:00<00:00, 164kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 810kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 514kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 357/357 [00:00<00:00, 141kB/s]\n",
            "Downloading pytorch_model.bin: 100% 526M/526M [00:05<00:00, 100MB/s] \n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "Running comparisons...\n",
            "100% 3/3 [00:01<00:00,  1.98it/s]\n",
            "Done\n",
            "\n",
            "CKA for EleutherAI/gpt-neo-1.3b\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 200/200 [00:00<00:00, 73.7kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.35k/1.35k [00:00<00:00, 496kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 798k/798k [00:01<00:00, 721kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 513kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 90.0/90.0 [00:00<00:00, 30.7kB/s]\n",
            "Downloading pytorch_model.bin: 100% 5.31G/5.31G [00:56<00:00, 94.7MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:02<00:00,  1.38it/s]\n",
            "Done\n",
            "\n",
            "CKA for EleutherAI/pythia-70m\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 394/394 [00:00<00:00, 135kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 2.11M/2.11M [00:01<00:00, 1.57MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 34.5kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 534/534 [00:00<00:00, 197kB/s]\n",
            "Downloading pytorch_model.bin: 100% 166M/166M [00:01<00:00, 90.0MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:00<00:00,  4.63it/s]\n",
            "Done\n",
            "\n",
            "CKA for EleutherAI/pythia-160m\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 394/394 [00:00<00:00, 139kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 2.11M/2.11M [00:01<00:00, 1.57MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 35.0kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 536/536 [00:00<00:00, 197kB/s]\n",
            "Downloading pytorch_model.bin: 100% 375M/375M [00:00<00:00, 463MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:01<00:00,  2.69it/s]\n",
            "Done\n",
            "\n",
            "CKA for EleutherAI/pythia-410m\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 394/394 [00:00<00:00, 115kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 2.11M/2.11M [00:01<00:00, 1.57MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 33.1kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 537/537 [00:00<00:00, 197kB/s]\n",
            "Downloading pytorch_model.bin: 100% 911M/911M [00:09<00:00, 92.3MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:02<00:00,  1.34it/s]\n",
            "Done\n",
            "\n",
            "CKA for EleutherAI/pythia-1b\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 394/394 [00:00<00:00, 134kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 2.11M/2.11M [00:01<00:00, 1.59MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 33.4kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 536/536 [00:00<00:00, 196kB/s]\n",
            "Downloading pytorch_model.bin: 100% 2.09G/2.09G [00:22<00:00, 92.0MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:01<00:00,  1.97it/s]\n",
            "Done\n",
            "\n",
            "CKA for EleutherAI/pythia-1.4b\n",
            "Loading  model...\n",
            "Downloading (…)okenizer_config.json: 100% 394/394 [00:00<00:00, 136kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 2.11M/2.11M [00:01<00:00, 1.56MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 33.3kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 537/537 [00:00<00:00, 200kB/s]\n",
            "Downloading pytorch_model.bin: 100% 2.93G/2.93G [00:21<00:00, 135MB/s]\n",
            "Running comparisons...\n",
            "100% 3/3 [00:02<00:00,  1.34it/s]\n",
            "Done\n",
            "\n",
            "\n",
            "Score dict summary:\n",
            "{'eleutherai/gpt-neo-125m': 'This model predicted 8/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.7983 while the mean p_true was 0.0234', 'eleutherai/gpt-neo-1.3b': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9925 while the mean p_true was 0.5798', 'eleutherai/pythia-70m': 'This model predicted 6/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.7077 while the mean p_true was 0.0158', 'eleutherai/pythia-160m': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9444 while the mean p_true was 0.0483', 'eleutherai/pythia-410m': 'This model predicted 8/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.8514 while the mean p_true was 0.1627', 'eleutherai/pythia-1b': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9941 while the mean p_true was 0.4998', 'eleutherai/pythia-1.4b': 'This model predicted 9/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.9729 while the mean p_true was 0.5957'}\n"
          ]
        }
      ],
      "source": [
        "!python run_cka.py configs.tests.eleutherai_v0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WXwAZfHMgAY"
      },
      "source": [
        "## Notebook usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVCMUZJXV0mq"
      },
      "outputs": [],
      "source": [
        "from run_cka import main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFy4ijAPc_mS"
      },
      "source": [
        "### gpt2 example with \"verbosity\" turned on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXTO0xG4Zzx5",
        "outputId": "cf07f45c-a9b5-4d0c-99ea-82f7e72b0a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for distilgpt2\n",
            "Loading  model...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "Running comparisons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 1/3 [00:00<00:01,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\tcontext... The 2020 Olympics were held in\n",
            "\ttokenized_context ids... tensor([[  464, 12131, 14935,   547,  2714,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... The 2020 Olympics were held in\n",
            "\tdecoded target id...  Tokyo\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... The 2020 Olympics were held in\n",
            "\ttokenized_context ids... tensor([[  464, 12131, 14935,   547,  2714,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... The 2020 Olympics were held in\n",
            "\tdecoded target id...  London\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... The 2020 Olympics were held in\n",
            "\ttokenized_context ids... tensor([[  464, 12131, 14935,   547,  2714,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... The 2020 Olympics were held in\n",
            "\tdecoded target id...  Tokyo\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... The 2020 Olympics were held in\n",
            "\ttokenized_context ids... tensor([[  464, 12131, 14935,   547,  2714,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... The 2020 Olympics were held in\n",
            "\tdecoded target id...  Berlin\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... The 2020 Olympics were held in\n",
            "\ttokenized_context ids... tensor([[  464, 12131, 14935,   547,  2714,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... The 2020 Olympics were held in\n",
            "\tdecoded target id...  Tokyo\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... The 2020 Olympics were held in\n",
            "\ttokenized_context ids... tensor([[  464, 12131, 14935,   547,  2714,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... The 2020 Olympics were held in\n",
            "\tdecoded target id...  Chicago\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Operation Overlord took place in\n",
            "\ttokenized_context ids... tensor([[32180,  3827, 10572,  1718,  1295,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Operation Overlord took place in\n",
            "\tdecoded target id...  Normandy\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Operation Overlord took place in\n",
            "\ttokenized_context ids... tensor([[32180,  3827, 10572,  1718,  1295,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Operation Overlord took place in\n",
            "\tdecoded target id...  Manila\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Operation Overlord took place in\n",
            "\ttokenized_context ids... tensor([[32180,  3827, 10572,  1718,  1295,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Operation Overlord took place in\n",
            "\tdecoded target id...  Normandy\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Operation Overlord took place in\n",
            "\ttokenized_context ids... tensor([[32180,  3827, 10572,  1718,  1295,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Operation Overlord took place in\n",
            "\tdecoded target id...  Santiago\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Operation Overlord took place in\n",
            "\ttokenized_context ids... tensor([[32180,  3827, 10572,  1718,  1295,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Operation Overlord took place in\n",
            "\tdecoded target id...  Normandy\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\tcontext... Operation Overlord took place in\n",
            "\ttokenized_context ids... tensor([[32180,  3827, 10572,  1718,  1295,   287]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Operation Overlord took place in\n",
            "\tdecoded target id...  Baghdad\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Steve Jobs is the founder of\n",
            "\ttokenized_context ids... tensor([[19206, 19161,   318,   262,  9119,   286]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Steve Jobs is the founder of\n",
            "\tdecoded target id...  Apple\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Steve Jobs is the founder of\n",
            "\ttokenized_context ids... tensor([[19206, 19161,   318,   262,  9119,   286]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Steve Jobs is the founder of\n",
            "\tdecoded target id...  Microsoft\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Steve Jobs is the founder of\n",
            "\ttokenized_context ids... tensor([[19206, 19161,   318,   262,  9119,   286]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Steve Jobs is the founder of\n",
            "\tdecoded target id...  Apple\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Steve Jobs is the founder of\n",
            "\ttokenized_context ids... tensor([[19206, 19161,   318,   262,  9119,   286]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Steve Jobs is the founder of\n",
            "\tdecoded target id...  Oracle\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Steve Jobs is the founder of\n",
            "\ttokenized_context ids... tensor([[19206, 19161,   318,   262,  9119,   286]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Steve Jobs is the founder of\n",
            "\tdecoded target id...  Apple\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "\n",
            "\tcontext... Steve Jobs is the founder of\n",
            "\ttokenized_context ids... tensor([[19206, 19161,   318,   262,  9119,   286]], device='cuda:0')\n",
            "\tdecoded tokenized_context... Steve Jobs is the founder of\n",
            "\tdecoded target id...  Intel\n",
            "\tmost probable prediction id decoded...  the\n",
            "\n",
            "Done\n",
            "\n",
            "{'distilgpt2': [{\"The 2020 Olympics were held in ['Tokyo', 'London']\": {'p_true': 0.01068115234375, 'p_false': 0.0478515625, 'p_true - p_false': -0.03717041015625, 'p_true > p_false': 'False', 'p_true / (p_true + p_false)': 0.1824817206487955}}, {\"The 2020 Olympics were held in ['Tokyo', 'Berlin']\": {'p_true': 0.01068115234375, 'p_false': 0.006084442138671875, 'p_true - p_false': 0.004596710205078125, 'p_true > p_false': 'True', 'p_true / (p_true + p_false)': 0.6370872195481408}}, {\"The 2020 Olympics were held in ['Tokyo', 'Chicago']\": {'p_true': 0.01068115234375, 'p_false': 0.0015382766723632812, 'p_true - p_false': 0.009142875671386719, 'p_true > p_false': 'True', 'p_true / (p_true + p_false)': 0.8741115144210135}}, {\"Operation Overlord took place in ['Normandy', 'Manila']\": {'p_true': 0.0007376670837402344, 'p_false': 0.00019848346710205078, 'p_true - p_false': 0.0005391836166381836, 'p_true > p_false': 'True', 'p_true / (p_true + p_false)': 0.7879706991247797}}, {\"Operation Overlord took place in ['Normandy', 'Santiago']\": {'p_true': 0.0007376670837402344, 'p_false': 6.860494613647461e-05, 'p_true - p_false': 0.0006690621376037598, 'p_true > p_false': 'True', 'p_true / (p_true + p_false)': 0.9148995716214006}}, {\"Operation Overlord took place in ['Normandy', 'Baghdad']\": {'p_true': 0.0007376670837402344, 'p_false': 0.0002713203430175781, 'p_true - p_false': 0.00046634674072265625, 'p_true > p_false': 'True', 'p_true / (p_true + p_false)': 0.7310891625467892}}, {\"Steve Jobs is the founder of ['Apple', 'Microsoft']\": {'p_true': 0.01404571533203125, 'p_false': 0.01201629638671875, 'p_true - p_false': 0.0020294189453125, 'p_true > p_false': 'True', 'p_true / (p_true + p_false)': 0.5389342194403219}}, {\"Steve Jobs is the founder of ['Apple', 'Oracle']\": {'p_true': 0.01404571533203125, 'p_false': 0.0016260147094726562, 'p_true - p_false': 0.012419700622558594, 'p_true > p_false': 'True', 'p_true / (p_true + p_false)': 0.8962447880601381}}, {\"Steve Jobs is the founder of ['Apple', 'Intel']\": {'p_true': 0.01404571533203125, 'p_false': 0.004283905029296875, 'p_true - p_false': 0.009761810302734375, 'p_true > p_false': 'True', 'p_true / (p_true + p_false)': 0.7662847016088723}}]}\n",
            "{'distilgpt2': [{\"The 2020 Olympics were held in ['Tokyo', 'London']\": {'p_true > p_false': 'False'}}, {\"The 2020 Olympics were held in ['Tokyo', 'Berlin']\": {'p_true > p_false': 'True'}}, {\"The 2020 Olympics were held in ['Tokyo', 'Chicago']\": {'p_true > p_false': 'True'}}, {\"Operation Overlord took place in ['Normandy', 'Manila']\": {'p_true > p_false': 'True'}}, {\"Operation Overlord took place in ['Normandy', 'Santiago']\": {'p_true > p_false': 'True'}}, {\"Operation Overlord took place in ['Normandy', 'Baghdad']\": {'p_true > p_false': 'True'}}, {\"Steve Jobs is the founder of ['Apple', 'Microsoft']\": {'p_true > p_false': 'True'}}, {\"Steve Jobs is the founder of ['Apple', 'Oracle']\": {'p_true > p_false': 'True'}}, {\"Steve Jobs is the founder of ['Apple', 'Intel']\": {'p_true > p_false': 'True'}}]}\n",
            "{'distilgpt2': 'This model predicted 8/9 facts at a higher prob than the given counterfactual. The mean p_true / (p_true + p_false) was 0.7032 while the mean p_true was 0.0085'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    \"models\": [\n",
        "        \"distilgpt2\",\n",
        "    ],\n",
        "    \"input_information\": {\n",
        "        \"0\": {\n",
        "            \"stem\": \"The 2020 Olympics were held in\",\n",
        "            \"true\": \"Tokyo\",\n",
        "            \"false\": [\"London\", \"Berlin\", \"Chicago\"],\n",
        "        },\n",
        "        \"1\": {\n",
        "            \"stem\": \"Operation Overlord took place in\",\n",
        "            \"true\": \"Normandy\",\n",
        "            \"false\": [\"Manila\", \"Santiago\", \"Baghdad\"],\n",
        "        },\n",
        "        \"2\": {\n",
        "            \"stem\": \"Steve Jobs is the founder of\",\n",
        "            \"true\": \"Apple\",\n",
        "            \"false\": [\"Microsoft\", \"Oracle\", \"Intel\"],\n",
        "        },\n",
        "    },\n",
        "    \"verbosity\": True,\n",
        "}\n",
        "\n",
        "score_dicts = main(config)\n",
        "\n",
        "print(score_dicts[0])\n",
        "print(score_dicts[1])\n",
        "print(score_dicts[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGFcOIO0XU6O"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
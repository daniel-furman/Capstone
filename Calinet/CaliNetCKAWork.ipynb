{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "X7ddC32v8Tbu",
        "YT_ITmY-8GZj",
        "5z8hPESW8Jdl",
        "1SmdsPaE8OGP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "uccv2X7WeJGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4gro-sOZz-O",
        "outputId": "009db0eb-d556-4b21-8097-82ef2c00b7dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper for T5Tokenizer\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERTUnbyRmASe",
        "outputId": "80f07645-9d48-41c3-801c-d24375385705"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3a0gYtW0uBPD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "prUx8libwb5R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models"
      ],
      "metadata": {
        "id": "LVCq8I6R7-TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load t5-small\n",
        "model_name = \"t5-small\"\n",
        "\n",
        "t5_small_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "t5_small_model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Mdy22xuNDJ",
        "outputId": "eb5015cf-f2a5-4c4b-e965-849fa4fe9022"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load flan-t5-small\n",
        "model_name = \"google/flan-t5-small\"\n",
        "\n",
        "flan_t5_small_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "flan_t5_small_model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "Fe_J1tWuf5JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load flan-t5-large\n",
        "model_name = \"google/flan-t5-large\"\n",
        "\n",
        "flan_t5_large_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "flan_t5_large_model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "yqJOHr1nimn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dmr6W7vZwPt"
      },
      "outputs": [],
      "source": [
        "# load gpt2\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "gpt_2_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "gpt_2_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load distilgpt2 - https://huggingface.co/distilgpt2\n",
        "model_name = \"distilgpt2\"\n",
        "\n",
        "distilgpt2_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "distilgpt2_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "BC95NHH8-JU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load gpt2-large - \n",
        "model_name = \"gpt2-large\"\n",
        "\n",
        "gpt2_large_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "gpt2_large_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "wDj2dmviZ7_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5 Work (reproducing/tweaking [existing CKA method](https://github.com/dqxiu/CaliNet/blob/master/cka/assessing_score.py))"
      ],
      "metadata": {
        "id": "mZ3qkD5meSgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probe_t5(model,input_ids, target):\n",
        "    # T5 is an encoder-decoder mode, so we have to provide both input_ids and decoder_input_ids to the model\n",
        "    # this is in contrast to BERT whose transformer follows the original attention paper and is 'encoder' only\n",
        "    # and the GPT-family of models which is decoder only (which really is quite similar to BERT other than the fact that)\n",
        "    # BERT can optimize reps based on the entire context before/after a word\n",
        "    # while GPT models are auto-regressive or 'causal' so they can only look at tokens that precede a given word \n",
        "    # the input id's correspond to the \n",
        "    # where 0 is the corresponding id for <pad>\n",
        "    # and 32099 is the corresponding id for <extra_id_0>\n",
        "    # this <extra_id_0> essentially stands in for the 'blank' predictor token\n",
        "    # which is how we'll solicit text-generation\n",
        "    outputs = model(input_ids=input_ids, decoder_input_ids=torch.tensor([[0, 32099]],device='cuda:0'),\n",
        "                    output_hidden_states=True, return_dict=True)\n",
        "    \n",
        "    # outputs contains:\n",
        "      # logits -- Prediction scores of the language modeling head)\n",
        "        # unnormalized scores for each possible token at the masked token position\n",
        "      # past_key_values -- Contains pre-computed hidden-states (key and values in the attention blocks) \n",
        "      # decoder_hidden_states  -- Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
        "      # encoder_last_hidden_state -- Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
        "      # encoder_hidden_states -- Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
        "    \n",
        "    # torch.Size([1, 2, 32128]) \n",
        "    # 1 x 2 x 32128 because \n",
        "    # we have batch_size of 1\n",
        "    # and a sequence length of 2\n",
        "    # and the vocab size for t5 is 32128\n",
        "\n",
        "    # torch.Size([32128])\n",
        "    # We have batch size of 1, so grab that, then, \n",
        "    # Take the entire last matrix which corresponds to the last layer\n",
        "    logits = outputs['logits'][0, -1]\n",
        "\n",
        "    # convert our prediction scores to a probability distribution with softmax\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # originally, they had \n",
        "    # torch.reshape(probs, (-1,)).detach().cpu().numpy()\n",
        "    # but that reshape line doesn't do anything since probs is already just 1-dimension (= to the embedding size)\n",
        "    # in this case, for every token in T5's vocabulary, we're getting.. \n",
        "    # a prediction that the hidden token is that word?\n",
        "    probs = probs.detach().cpu().numpy()\n",
        "\n",
        "    # so let's just grab that by its given\n",
        "    return probs[target.item()]"
      ],
      "metadata": {
        "id": "BpdvRZituHnF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test out T5 Implementation"
      ],
      "metadata": {
        "id": "0DYBtsxM8gdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t5_small"
      ],
      "metadata": {
        "id": "uH9UwRoP8Ra5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_true = \"The 2020 Olympic Games took place in <extra_id_0> .\"\n",
        "target_token = \"Tokyo\"\n",
        "\n",
        "target = t5_small_tokenizer.encode(target_token, return_tensors=\"pt\").to(device)[0][0]\n",
        "            \n",
        "input_ids = t5_small_tokenizer.encode(src_true, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# what are we looking for, exactly?\n",
        "tokenized_target_of_interest = t5_small_tokenizer.convert_ids_to_tokens([target.detach().cpu().numpy()])\n",
        "\n",
        "P_true = probe_t5(t5_small_model,input_ids, target) \n",
        "\n",
        "print(f\"according to t5_small, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_true}\")"
      ],
      "metadata": {
        "id": "z9xoM1y1p1hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edfda3d-2bc0-405a-e30a-ece33c3c6462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to t5_small, prob. the next token in the input sequence is ['▁Tokyo'] = 0.007489456795156002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_true = \"The 2020 Olympic Games took place in <extra_id_0> .\"\n",
        "target_token = \"Berlin\"\n",
        "\n",
        "target = t5_small_tokenizer.encode(target_token, return_tensors=\"pt\").to(device)[0][0]\n",
        "            \n",
        "input_ids = t5_small_tokenizer.encode(src_true, return_tensors=\"pt\").to(device)\n",
        "\n",
        "tokenized_target_of_interest = t5_small_tokenizer.convert_ids_to_tokens([target.detach().cpu().numpy()])\n",
        "\n",
        "P_false = probe_t5(t5_small_model,input_ids, target) \n",
        "\n",
        "print(f\"according to t5_small, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_false}\")"
      ],
      "metadata": {
        "id": "vLD9cy1Uz-Cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0904006-a040-4803-9916-39f471111e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to t5_small, prob. the next token in the input sequence is ['▁Berlin'] = 0.003681196365505457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P_true > P_false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXSFpPfRfJQC",
        "outputId": "0166d422-1e61-4ff1-96af-2b826ac97bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### flan_t5-small"
      ],
      "metadata": {
        "id": "X7ddC32v8Tbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_true = \"The 2020 Olympic Games took place in <extra_id_0> .\"\n",
        "target_token = \"Tokyo\"\n",
        "\n",
        "target = flan_t5_small_tokenizer.encode(target_token, return_tensors=\"pt\").to(device)[0][0]\n",
        "            \n",
        "input_ids = flan_t5_small_tokenizer.encode(src_true, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# what are we looking for, exactly?\n",
        "tokenized_target_of_interest = flan_t5_small_tokenizer.convert_ids_to_tokens([target.detach().cpu().numpy()])\n",
        "\n",
        "P_true = probe_t5(flan_t5_small_model,input_ids, target) \n",
        "\n",
        "print(f\"according to flan_t5_small, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_true}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4IEJ3wEiU9v",
        "outputId": "421d1e8a-6790-4e26-f6b7-c75db8178dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to flan_t5_small, prob. the next token in the input sequence is ['▁Tokyo'] = 1.542711106594652e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_true = \"The 2020 Olympic Games took place in <extra_id_0> .\"\n",
        "target_token = \"Berlin\"\n",
        "\n",
        "target = flan_t5_small_tokenizer.encode(target_token, return_tensors=\"pt\").to(device)[0][0]\n",
        "            \n",
        "input_ids = flan_t5_small_tokenizer.encode(src_true, return_tensors=\"pt\").to(device)\n",
        "\n",
        "tokenized_target_of_interest = flan_t5_small_tokenizer.convert_ids_to_tokens([target.detach().cpu().numpy()])\n",
        "\n",
        "P_false = probe_t5(flan_t5_small_model,input_ids, target) \n",
        "\n",
        "print(f\"according to flan_t5_small, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_false}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeD3i4UGiZWh",
        "outputId": "667a6ce3-6952-4de9-f494-3a6ca1cf8f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to flan_t5_small, prob. the next token in the input sequence is ['▁Berlin'] = 1.2637829058803618e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P_true > P_false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGElpuQuif4U",
        "outputId": "acb2ef29-15d6-4b43-a65d-965ee3fd179a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### flan-t5-large"
      ],
      "metadata": {
        "id": "q9d_1UxMihcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_true = \"The 2020 Olympic Games took place in <extra_id_0> .\"\n",
        "target_token = \"Tokyo\"\n",
        "\n",
        "target = flan_t5_large_tokenizer.encode(target_token, return_tensors=\"pt\").to(device)[0][0]\n",
        "            \n",
        "input_ids = flan_t5_large_tokenizer.encode(src_true, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# what are we looking for, exactly?\n",
        "tokenized_target_of_interest = flan_t5_large_tokenizer.convert_ids_to_tokens([target.detach().cpu().numpy()])\n",
        "\n",
        "P_true = probe_t5(flan_t5_large_model,input_ids, target) \n",
        "\n",
        "print(f\"according to flan_t5_large, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_true}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYi7erMai8nc",
        "outputId": "88f9d880-898d-4fbb-d3b0-0066f7d64d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to flan_t5_large, prob. the next token in the input sequence is ['▁Tokyo'] = 0.0034607253037393093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_true = \"The 2020 Olympic Games took place in <extra_id_0> .\"\n",
        "target_token = \"Berlin\"\n",
        "\n",
        "target = flan_t5_large_tokenizer.encode(target_token, return_tensors=\"pt\").to(device)[0][0]\n",
        "            \n",
        "input_ids = flan_t5_large_tokenizer.encode(src_true, return_tensors=\"pt\").to(device)\n",
        "\n",
        "tokenized_target_of_interest = flan_t5_large_tokenizer.convert_ids_to_tokens([target.detach().cpu().numpy()])\n",
        "\n",
        "P_false = probe_t5(flan_t5_large_model,input_ids, target) \n",
        "\n",
        "print(f\"according to flan_t5_large, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_false}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9MgUPI-jVo-",
        "outputId": "45fb3243-8804-4677-8b22-fe761402a26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to flan_t5_large, prob. the next token in the input sequence is ['▁Berlin'] = 0.000330989743815735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P_true > P_false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEg6TyTmjm8m",
        "outputId": "52278be9-5da0-4d19-a2cb-6f8c2d0e3d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT work for CKA (adapting CKA method)"
      ],
      "metadata": {
        "id": "KhlH8d-HebTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  model: a pretrained model pulled in from HuggingFace; in this case \n",
        "    the architecture we'll use is inferred from the name or the path of the pretrained model \n",
        "    we supplied in the from_pretrained() method called in the setup for this notebook\n",
        "  see more - https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM\n",
        "  input_ids: the indices (in gpt's vocabulary) of our left-context tokens\n",
        "  target: the index (in gpt's vocabulary) of the token we're gathering a prediction for\n",
        "\n",
        "  return: a float indicating the likelihood of the target following the left-context according to the model\n",
        "    in case of error, return None\n",
        "'''\n",
        "def probe_gpt(model, input_ids, target):\n",
        "\n",
        "  # ensure we're only asking for a single token prediction \n",
        "  if len(target) > 1:\n",
        "    # default to the very first token that get's predicted\n",
        "    # e.g. in the case of Tokyo, which gets split into <Tok> <yo>,\n",
        "    # we can presume that if the model produces a highprob for <Tok> that's sufficient enough to\n",
        "    # conclude that it's individual prob is fairly representative to\n",
        "    # the likelihood that Tokyou would be predicted\n",
        "    target = target[0]\n",
        "  \n",
        "  # sanity check - do a conversion that tells us the exact \"token\" that's being predicted on\n",
        "  # print(model.convert_)\n",
        "  \n",
        "  # grab value\n",
        "  target_scalar = target.detach().cpu().numpy()\n",
        "\n",
        "  # use model to solicit a prediction\n",
        "  outputs = model(input_ids=input_ids, output_hidden_states=True, return_dict=True)\n",
        "\n",
        "  # shape of 50257 which corresponds to the vocab size of GPT\n",
        "  # every token in GPT's vocab gets a representative prediction from the model\n",
        "  logits = outputs['logits'][0, -1]\n",
        "\n",
        "  # grab those probabilities\n",
        "  probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "  probs = list(probs.detach().cpu().numpy())\n",
        "\n",
        "  # double check weird-ness before accessing prob\n",
        "  if len(probs) < target:\n",
        "    return None\n",
        "  \n",
        "  # return the likelihood that our stipulated target would follow the context, according to the model\n",
        "  try:\n",
        "     return np.take(probs, [target_scalar])[0]\n",
        "  except IndexError:\n",
        "\n",
        "    print(f\"target index not in model vocabulary scope; raising IndexError\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "_pQXiZ1ebCX3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test out GPT Implementation"
      ],
      "metadata": {
        "id": "liO5AafU77wG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-2"
      ],
      "metadata": {
        "id": "YT_ITmY-8GZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'The 2020 Olympic Games took place in '\n",
        "target_token = 'Tokyo'\n",
        "\n",
        "target = gpt_2_tokenizer.encode(target_token, return_tensors='pt').to(device)[0]\n",
        "\n",
        "# these input ids correspond to the tokenized representation of the words preceding the blank space we're soliciting a prediction for\n",
        "input_ids = gpt_2_tokenizer.encode(phrase, return_tensors='pt').to(device)\n",
        "\n",
        "# what are we looking for, exactly?\n",
        "tokenized_target_of_interest = gpt_2_tokenizer.convert_ids_to_tokens(list(target.detach().cpu().numpy()))[0]\n",
        "\n",
        "P_true = probe_gpt(gpt_2_model,input_ids, target)\n",
        "\n",
        "print(f\"according to gpt_2, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_true}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkv-YyZkZrtL",
        "outputId": "efc3c9ab-3c12-40a6-89fa-2ce6a1720537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to gpt_2, prob. the next token in the input sequence is Tok = 2.607206965876685e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'The 2020 Olympic Games took place in '\n",
        "target_token = 'Berlin'\n",
        "\n",
        "target = gpt_2_tokenizer.encode(target_token, return_tensors='pt').to(device)[0]\n",
        "\n",
        "input_ids = gpt_2_tokenizer.encode(phrase, return_tensors='pt').to(device)\n",
        "\n",
        "tokenized_target_of_interest = gpt_2_tokenizer.convert_ids_to_tokens(list(target.detach().cpu().numpy()))[0]\n",
        "\n",
        "P_false = probe_gpt(gpt_2_model,input_ids, target)\n",
        "\n",
        "print(f\"according to gpt_2, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_false}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKK-0ADraZar",
        "outputId": "a763d9b2-b765-474d-fd1a-50c2f9d5d6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to gpt_2, prob. the next token in the input sequence is Ber = 2.477075327078637e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P_true > P_false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emTTdJ1MfHap",
        "outputId": "c0ca5499-6f3e-4564-b814-6308329eaa57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### distill-gpt2"
      ],
      "metadata": {
        "id": "5z8hPESW8Jdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'The 2020 Olympic Games took place in '\n",
        "target_token = 'Tokyo'\n",
        "\n",
        "target = distilgpt2_tokenizer.encode(target_token, return_tensors='pt').to(device)[0]\n",
        "\n",
        "# these input ids correspond to the tokenized representation of the words preceding the blank space we're soliciting a prediction for\n",
        "input_ids = distilgpt2_tokenizer.encode(phrase, return_tensors='pt').to(device)\n",
        "\n",
        "# what are we looking for, exactly?\n",
        "tokenized_target_of_interest = distilgpt2_tokenizer.convert_ids_to_tokens(list(target.detach().cpu().numpy()))[0]\n",
        "\n",
        "P_true = probe_gpt(distilgpt2_model,input_ids, target)\n",
        "\n",
        "print(f\"according to distilgpt2, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_true}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yZ5Dyj88LsL",
        "outputId": "d8f78d54-95dd-4afe-f2d2-ad4885346520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to distilgpt2, prob. the next token in the input sequence is Tok = 5.20289745509217e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'The 2020 Olympic Games took place in '\n",
        "target_token = 'Berlin'\n",
        "\n",
        "target = distilgpt2_tokenizer.encode(target_token, return_tensors='pt').to(device)[0]\n",
        "\n",
        "# these input ids correspond to the tokenized representation of the words preceding the blank space we're soliciting a prediction for\n",
        "input_ids = distilgpt2_tokenizer.encode(phrase, return_tensors='pt').to(device)\n",
        "\n",
        "# what are we looking for, exactly?\n",
        "tokenized_target_of_interest = distilgpt2_tokenizer.convert_ids_to_tokens(list(target.detach().cpu().numpy()))[0]\n",
        "\n",
        "P_false = probe_gpt(distilgpt2_model,input_ids, target)\n",
        "\n",
        "print(f\"according to distilgpt2, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_false}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f12xciQwZkAN",
        "outputId": "b45ad670-5cc7-4921-89ae-f461a52dcf29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to distilgpt2, prob. the next token in the input sequence is Ber = 3.7727363633166533e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P_true > P_false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePBji4mJZouA",
        "outputId": "81366811-3913-4886-fba0-afe58bf99567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gpt2_large"
      ],
      "metadata": {
        "id": "1SmdsPaE8OGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'The 2020 Olympic Games took place in '\n",
        "target_token = 'Tokyo'\n",
        "\n",
        "target = gpt2_large_tokenizer.encode(target_token, return_tensors='pt').to(device)[0]\n",
        "\n",
        "# these input ids correspond to the tokenized representation of the words preceding the blank space we're soliciting a prediction for\n",
        "input_ids = gpt2_large_tokenizer.encode(phrase, return_tensors='pt').to(device)\n",
        "\n",
        "tokenized_target_of_interest = gpt2_large_tokenizer.convert_ids_to_tokens(list(target.detach().cpu().numpy()))[0]\n",
        "\n",
        "P_true = probe_gpt(gpt2_large_model,input_ids, target)\n",
        "\n",
        "print(f\"according to gp2_large, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_true}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7HuQpEX8PVc",
        "outputId": "93cdea3d-1508-4301-f2a7-479cba6f31f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to gp2_large, prob. the next token in the input sequence is Tok = 5.20289745509217e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'The 2020 Olympic Games took place in '\n",
        "target_token = 'Berlin'\n",
        "\n",
        "target = gpt2_large_tokenizer.encode(target_token, return_tensors='pt').to(device)[0]\n",
        "\n",
        "# these input ids correspond to the tokenized representation of the words preceding the blank space we're soliciting a prediction for\n",
        "input_ids = gpt2_large_tokenizer.encode(phrase, return_tensors='pt').to(device)\n",
        "\n",
        "tokenized_target_of_interest = gpt2_large_tokenizer.convert_ids_to_tokens(list(target.detach().cpu().numpy()))[0]\n",
        "\n",
        "P_false = probe_gpt(gpt2_large_model,input_ids, target)\n",
        "\n",
        "print(f\"according to gp2_large, prob. the next token in the input sequence is {tokenized_target_of_interest} = {P_false}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ED-6NxbbHVK",
        "outputId": "50ee3e8f-96f5-4c4d-f220-39833a50b4fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "according to gp2_large, prob. the next token in the input sequence is Ber = 3.7727363633166533e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P_true > P_false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Txt9un3bL5R",
        "outputId": "fae8f79b-158a-4d69-9c85-50f28704d955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-wise comparison\n",
        "\n",
        "* we should be able to do the following\n",
        "  * input a set of models we want to evaluate\n",
        "  * input an expression of interest\n",
        "  * input a 'true' next-token alonside a false\n",
        "  * and get an output report that contains..\n",
        "    * the 'result' ie is true > false\n",
        "    * the probabilities of both of those values\n",
        "  * running this method over a large set of positive/negative pairings should result in a large pool of information that can be used to compare model-families\n",
        "  * we can also look at the relative 'certainty' across different models (at least in orders of magnitude)"
      ],
      "metadata": {
        "id": "DfbssUuWj-LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, write helper to pull a pretrained LM and tokenizer off the shelf\n",
        "def get_model_and_tokenizer(model_name):\n",
        "  \n",
        "  if \"t5\" in model_name.lower():\n",
        "    return T5Tokenizer.from_pretrained(model_name), T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "  elif \"gpt\" in model_name.lower():\n",
        "    return AutoTokenizer.from_pretrained(model_name), AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "  elif \"bert\" in model_name.lower():\n",
        "    return 0"
      ],
      "metadata": {
        "id": "k5iSnp0clNff"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_probe_function(prefix, probe_functions):\n",
        "  for func in probe_functions:\n",
        "    if prefix.lower() in func.__name__:\n",
        "      return func"
      ],
      "metadata": {
        "id": "DvMrDCIXri7m"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probe_functions = [probe_t5, probe_gpt]"
      ],
      "metadata": {
        "id": "5-GUQV-NrwmO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(model_name_list, probe_functions, input_pairings):\n",
        "\n",
        "  score_dict = {}\n",
        "  for model_name in model_name_list:\n",
        "\n",
        "    print(f\"Running comparisons for {model_name}\")\n",
        "\n",
        "    # get proper model and tokenizer\n",
        "    tokenizer, model = get_model_and_tokenizer(model_name)\n",
        "    # establish prefix\n",
        "    prefix = \"\"\n",
        "    probe_func = None\n",
        "\n",
        "    # get correct CKA function\n",
        "    if \"t5\" in model_name.lower():\n",
        "      prefix = \"t5\"\n",
        "      probe_func = get_probe_function(prefix, probe_functions) \n",
        "      \n",
        "    elif \"gpt\" in model_name.lower():\n",
        "      prefix = \"gpt\"\n",
        "      probe_func = get_probe_function(prefix, probe_functions)\n",
        "\n",
        "    # iterate over context/entity pairings\n",
        "    # input_pairings is a dict\n",
        "    # context is a plain string (since our context's will be unique)\n",
        "    # and entities is a list containing, in the first slot, the true value for the statement\n",
        "    # and in the subsequent slots, incorrect information\n",
        "\n",
        "    for context, entities in input_pairings.items():\n",
        "      entity_count = 0\n",
        "      p_true = 0.0\n",
        "      p_false = 0.0\n",
        "\n",
        "      if prefix == \"t5\":\n",
        "        context += \" <extra_id_0> .\"\n",
        "      \n",
        "      for entity in entities:\n",
        "        target = None\n",
        "        if prefix == \"t5\":\n",
        "          target = tokenizer.encode(entity, return_tensors=\"pt\").to(device)[0][0]\n",
        "        elif prefix == \"gpt\":\n",
        "          target = tokenizer.encode(entity, return_tensors='pt').to(device)[0]\n",
        "        \n",
        "        # tokenize context\n",
        "        input_ids = tokenizer.encode(context, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # call probe function\n",
        "        model_prob = probe_func(model,input_ids, target) \n",
        "\n",
        "        if entity_count == 0:\n",
        "          p_true = model_prob\n",
        "\n",
        "        else:\n",
        "          p_false += model_prob\n",
        "\n",
        "        entity_count += 1\n",
        "\n",
        "      p_false /= entity_count -1\n",
        "\n",
        "      score_dict[model_name.lower() + \": \" + context] = (p_true, p_false)\n",
        "\n",
        "  return score_dict"
      ],
      "metadata": {
        "id": "QzWkEzLVj64M"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_information =  {'The 2020 Olympics were held in': ['Tokyo', 'Berlin'], 'Operation Overlord took place in': ['Normandy', 'Manila']}"
      ],
      "metadata": {
        "id": "bWNQfWATwgqg"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models(['distilgpt2', 't5-small'], [probe_t5, probe_gpt], input_information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsrWFCwUvf_C",
        "outputId": "012d2bf9-1320-4a50-bc2f-81d6f37f87ae"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running comparisons for distilgpt2\n",
            "Running comparisons for t5-small\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'distilgpt2: The 2020 Olympics were held in': (4.077246e-07,\n",
              "  4.857664492874392e-08),\n",
              " 'distilgpt2: Operation Overlord took place in': (6.5069177e-09,\n",
              "  1.0333457822753189e-07),\n",
              " 't5-small: The 2020 Olympics were held in <extra_id_0> .': (0.006753775,\n",
              "  0.0037354647647589445),\n",
              " 't5-small: Operation Overlord took place in <extra_id_0> .': (0.00078862614,\n",
              "  0.00026356257149018347)}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ]
}
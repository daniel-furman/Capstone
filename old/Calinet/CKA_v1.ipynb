{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99b21ab44ff04b73b8acada7c591f775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_489a18d9e96f41759342dcad1116c01a",
              "IPY_MODEL_73d29e29da104b94a42b320ffa991c21",
              "IPY_MODEL_c635d0e0e4a64e69b50ff79ee8011622"
            ],
            "layout": "IPY_MODEL_9c33f15d54a9495cbfe18a045cf61c62"
          }
        },
        "489a18d9e96f41759342dcad1116c01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fe4ff948f694f8083dd8228ef7870c8",
            "placeholder": "​",
            "style": "IPY_MODEL_1e67551fc89043b0b0713069113eb58f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "73d29e29da104b94a42b320ffa991c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_581e028b9a244cb48bc860118138739a",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24e63ef84db8458b959e69cb4e3bcc89",
            "value": 8
          }
        },
        "c635d0e0e4a64e69b50ff79ee8011622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1d5c7f805714c7eb79d1bbecc0529d0",
            "placeholder": "​",
            "style": "IPY_MODEL_7753f9b12b2b4251abda103c88493cd1",
            "value": " 8/8 [00:44&lt;00:00,  5.53s/it]"
          }
        },
        "9c33f15d54a9495cbfe18a045cf61c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fe4ff948f694f8083dd8228ef7870c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e67551fc89043b0b0713069113eb58f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "581e028b9a244cb48bc860118138739a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e63ef84db8458b959e69cb4e3bcc89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1d5c7f805714c7eb79d1bbecc0529d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7753f9b12b2b4251abda103c88493cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "uccv2X7WeJGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system(\"pip install transformers sentencepiece accelerate bitsandbytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4gro-sOZz-O",
        "outputId": "e7a90f9a-3ac2-4495-dbb8-7db0e8bbfd4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "3yjnEaRtKd8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    set_seed,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        ")"
      ],
      "metadata": {
        "id": "3a0gYtW0uBPD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "j6lqNVBmhW-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "YE3mmQSxhVmO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "prUx8libwb5R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probe helper functions"
      ],
      "metadata": {
        "id": "mZ3qkD5meSgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probe_t5(model, input_ids, target):\n",
        "    \"\"\"\n",
        "    model: a pretrained google model pulled in from HuggingFace (ie. flan-t5-small,\n",
        "      flan-ul2, t5-small, etc.)\n",
        "    input_ids: the indices (in the vocabulary) of our left-context tokens\n",
        "    target: the index (in the vocabulary) of the token we're gathering a prediction for\n",
        "  \n",
        "    return: a float indicating the likelihood of the target following the left-context\n",
        "      according to the model in case of error, return None\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the model\n",
        "    outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        decoder_input_ids=torch.tensor([[0, 32099]], device=\"cuda:0\"),\n",
        "        output_hidden_states=True,\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "    # We have batch size of 1, so grab that, then,\n",
        "    # Take the entire last matrix which corresponds to the last layer\n",
        "    logits = outputs[\"logits\"][0, -1]\n",
        "\n",
        "    # convert our prediction scores to a probability distribution with softmax\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    probs = probs.detach().cpu().numpy()\n",
        "\n",
        "    return probs[target.item()]"
      ],
      "metadata": {
        "id": "BpdvRZituHnF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def probe_gpt(model, input_ids, target):\n",
        "    \"\"\"\n",
        "    model: a gpt pretrained model pulled in from HuggingFace\n",
        "    input_ids: the indices (in gpt's vocabulary) of our left-context tokens\n",
        "    target: the index (in gpt's vocabulary) of the token we're gathering a prediction for\n",
        "  \n",
        "    return: a float indicating the likelihood of the target following the left-context\n",
        "      according to the model in case of error, return None\n",
        "    \"\"\"\n",
        "\n",
        "    # ensure we're only asking for a single token prediction\n",
        "    if len(target) > 1:\n",
        "        # default to the very first token that get's predicted\n",
        "        # e.g. in the case of Tokyo, which gets split into <Tok> <yo>,\n",
        "        target = target[0]\n",
        "\n",
        "    # sanity check - do a conversion that tells us the exact \"token\" predicted on\n",
        "    # print(model.convert_)\n",
        "\n",
        "    # grab value\n",
        "    target_scalar = target.detach().cpu().numpy()\n",
        "\n",
        "    # use model to solicit a prediction\n",
        "    outputs = model(input_ids=input_ids, output_hidden_states=True, return_dict=True)\n",
        "\n",
        "    # shape of 50257 which corresponds to the vocab size of GPT\n",
        "    # every token in GPT's vocab gets a representative prediction from the model\n",
        "    logits = outputs[\"logits\"][0, -1]\n",
        "\n",
        "    # convert our prediction scores to a probability distribution with softmax\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    probs = list(probs.detach().cpu().numpy())\n",
        "\n",
        "    # double check weird-ness before accessing prob\n",
        "    if len(probs) < target:\n",
        "        return None\n",
        "\n",
        "    # return the likelihood that our stipulated target would follow the context,\n",
        "    # according to the model\n",
        "    try:\n",
        "        return np.take(probs, [target_scalar])[0]\n",
        "    except IndexError:\n",
        "\n",
        "        print(\"target index not in model vocabulary scope; raising IndexError\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "_pQXiZ1ebCX3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-wise comparison helper functions\n",
        "\n",
        "* we should be able to do the following\n",
        "  * input a set of models we want to evaluate\n",
        "  * input an expression of interest\n",
        "  * input a 'true' next-token alonside a false\n",
        "  * and get an output report that contains..\n",
        "    * the 'result' ie is true > false\n",
        "    * the probabilities of both of those values\n",
        "  * running this method over a large set of positive/negative pairings should result in a large pool of information that can be used to compare model-families\n",
        "  * we can also look at the relative 'certainty' across different models (at least in orders of magnitude)"
      ],
      "metadata": {
        "id": "DfbssUuWj-LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, write helper to pull a pretrained LM and tokenizer off the shelf\n",
        "def get_model_and_tokenizer(model_name):\n",
        "    if \"ul2\" in model_name.lower():\n",
        "        return T5Tokenizer.from_pretrained(\n",
        "            model_name\n",
        "        ), T5ForConditionalGeneration.from_pretrained(\n",
        "            model_name, load_in_8bit=True, device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    elif \"t5\" in model_name.lower():\n",
        "        return T5Tokenizer.from_pretrained(\n",
        "            model_name\n",
        "        ), T5ForConditionalGeneration.from_pretrained(\n",
        "            model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
        "        ).to(\n",
        "            device\n",
        "        )\n",
        "    elif \"gpt\" in model_name.lower():\n",
        "        return AutoTokenizer.from_pretrained(\n",
        "            model_name\n",
        "        ), AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
        "        ).to(\n",
        "            device\n",
        "        )"
      ],
      "metadata": {
        "id": "k5iSnp0clNff"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next, write a helper to pull a probe function for the given LM\n",
        "def get_probe_function(prefix):\n",
        "    probe_functions = [probe_t5, probe_gpt]\n",
        "    for func in probe_functions:\n",
        "        if prefix.lower() in func.__name__:\n",
        "            return func"
      ],
      "metadata": {
        "id": "DvMrDCIXri7m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lastly, write a wrapper function to compare models\n",
        "def compare_models(model_name_list, input_pairings):\n",
        "\n",
        "    score_dict = {}\n",
        "\n",
        "    for model_name in model_name_list:\n",
        "        print(f\"CKA for {model_name}\")\n",
        "        print(\"\\tLoading  model...\")\n",
        "\n",
        "        # get proper model and tokenizer\n",
        "        tokenizer, model = get_model_and_tokenizer(model_name)\n",
        "\n",
        "        print(\"\\tRunning comparisons...\")\n",
        "\n",
        "        # establish prefix\n",
        "        prefix = \"\"\n",
        "        probe_func = None\n",
        "\n",
        "        # get correct CKA function\n",
        "        if ((\"t5\" in model_name.lower()) or (\"ul2\" in model_name.lower())):\n",
        "            prefix = \"t5\"\n",
        "            probe_func = get_probe_function(prefix)\n",
        "\n",
        "        elif \"gpt\" in model_name.lower():\n",
        "            prefix = \"gpt\"\n",
        "            probe_func = get_probe_function(prefix)\n",
        "\n",
        "        # iterate over context/entity pairings\n",
        "        # input_pairings is a dict\n",
        "        # context is a plain string (since our context's will be unique)\n",
        "        # and entities is a list containing, in the first slot, the true\n",
        "        # value for the statement and in the subsequent slots, incorrect information\n",
        "\n",
        "        for context, entities in input_pairings.items():\n",
        "            entity_count = 0\n",
        "            p_true = 0.0\n",
        "            p_false = 0.0\n",
        "\n",
        "            if prefix == \"t5\":\n",
        "                context += \" <extra_id_0> .\"\n",
        "\n",
        "            for entity in entities:\n",
        "                target = None\n",
        "                if prefix == \"t5\":\n",
        "                    target = tokenizer.encode(\n",
        "                        entity,\n",
        "                        padding=\"longest\",\n",
        "                        max_length=512,\n",
        "                        truncation=True,\n",
        "                        return_tensors=\"pt\",\n",
        "                    ).to(device)[0][0]\n",
        "                elif prefix == \"gpt\":\n",
        "                    target = tokenizer.encode(entity, return_tensors=\"pt\").to(device)[0]\n",
        "\n",
        "                # tokenize context\n",
        "                input_ids = tokenizer.encode(\n",
        "                        context,\n",
        "                        return_tensors=\"pt\",\n",
        "                    ).to(device)\n",
        "\n",
        "                # call probe function\n",
        "                model_prob = probe_func(model, input_ids, target)\n",
        "\n",
        "                if entity_count == 0:\n",
        "                    p_true = model_prob\n",
        "\n",
        "                else:\n",
        "                    p_false += model_prob\n",
        "\n",
        "                entity_count += 1\n",
        "\n",
        "            p_false /= entity_count - 1\n",
        "            score_dict[model_name.lower() + \": \" + context] = {\n",
        "                'p_true': p_true,\n",
        "                'p_false': p_false,\n",
        "                'p_true - p_false': p_true - p_false,\n",
        "                'p_true > p_false': p_true > p_false\n",
        "            }\n",
        "\n",
        "        print(\"\\tDone\\n\")\n",
        "        del tokenizer\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return score_dict\n"
      ],
      "metadata": {
        "id": "QzWkEzLVj64M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test model-wise comparison"
      ],
      "metadata": {
        "id": "8WXwAZfHMgAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flans"
      ],
      "metadata": {
        "id": "RUEkCx59u6mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"models\": [\n",
        "        #\"google/flan-t5-small\",\n",
        "        #\"google/flan-t5-large\",\n",
        "        #\"google/flan-t5-xl\",\n",
        "        #\"google/flan-t5-xxl\",\n",
        "        \"google/flan-ul2\",\n",
        "    ],\n",
        "    \"input_information\": {\n",
        "        \"The 2020 Olympics were held in\": [\"Tokyo\", \"Berlin\"],\n",
        "        \"Operation Overlord took place in\": [\"Normandy\", \"Manila\"],\n",
        "        \"Steve Jobs is the founder of\": [\"Apple\", \"Microsoft\"]\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "13NVylxhao2Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_dict = compare_models(config[\"models\"], config[\"input_information\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "99b21ab44ff04b73b8acada7c591f775",
            "489a18d9e96f41759342dcad1116c01a",
            "73d29e29da104b94a42b320ffa991c21",
            "c635d0e0e4a64e69b50ff79ee8011622",
            "9c33f15d54a9495cbfe18a045cf61c62",
            "7fe4ff948f694f8083dd8228ef7870c8",
            "1e67551fc89043b0b0713069113eb58f",
            "581e028b9a244cb48bc860118138739a",
            "24e63ef84db8458b959e69cb4e3bcc89",
            "f1d5c7f805714c7eb79d1bbecc0529d0",
            "7753f9b12b2b4251abda103c88493cd1"
          ]
        },
        "id": "oVkmOngqRp2I",
        "outputId": "52faf013-a70d-4f5b-cad5-9a87ce031c59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for google/flan-ul2\n",
            "\tLoading  model...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99b21ab44ff04b73b8acada7c591f775"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tRunning comparisons...\n",
            "\tDone\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_dict\n"
      ],
      "metadata": {
        "id": "eN3ov6pRdR6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff96cd9d-6509-40f4-c210-b8c18bb057d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'google/flan-ul2: The 2020 Olympics were held in <extra_id_0> .': {'p_true': 0.7124,\n",
              "  'p_false': 0.0001531839370727539,\n",
              "  'p_true - p_false': 0.7122491598129272,\n",
              "  'p_true > p_false': True},\n",
              " 'google/flan-ul2: Operation Overlord took place in <extra_id_0> .': {'p_true': 0.2974,\n",
              "  'p_false': 0.0,\n",
              "  'p_true - p_false': 0.29736328125,\n",
              "  'p_true > p_false': True},\n",
              " 'google/flan-ul2: Steve Jobs is the founder of <extra_id_0> .': {'p_true': 0.4827,\n",
              "  'p_false': 0.00016295909881591797,\n",
              "  'p_true - p_false': 0.4825030565261841,\n",
              "  'p_true > p_false': True}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT2s"
      ],
      "metadata": {
        "id": "c-2jm5Zcu4YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"models\": [\n",
        "        #\"distilgpt2\", \n",
        "        #\"gpt2\", \n",
        "        #\"gpt2-medium\", \n",
        "        #\"gpt2-large\", \n",
        "        \"gpt2-xl\"\n",
        "    ],\n",
        "    \"input_information\": {\n",
        "        \"The 2020 Olympics were held in\": [\"Tokyo\", \"Berlin\"],\n",
        "        \"Operation Overlord took place in\": [\"Normandy\", \"Manila\"],\n",
        "        \"Steve Jobs is the founder of\": [\"Apple\", \"Microsoft\"]\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "a5eCEmAIdkbF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_dict = compare_models(config[\"models\"], config[\"input_information\"])"
      ],
      "metadata": {
        "id": "eajQsrTJdkbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782ea693-c85a-4d82-e557-ae70d3fd5d6c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKA for gpt2-xl\n",
            "\tLoading  model...\n",
            "\tRunning comparisons...\n",
            "\tDone\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_dict\n"
      ],
      "metadata": {
        "id": "6V_gYxAZdkbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f6521f-43d9-435c-9b84-5420307575ac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gpt2-xl: The 2020 Olympics were held in': {'p_true': 2.384e-05,\n",
              "  'p_false': 5.960464477539063e-08,\n",
              "  'p_true - p_false': 2.378225326538086e-05,\n",
              "  'p_true > p_false': True},\n",
              " 'gpt2-xl: Operation Overlord took place in': {'p_true': 0.0,\n",
              "  'p_false': 5.960464477539063e-08,\n",
              "  'p_true - p_false': -5.960464477539063e-08,\n",
              "  'p_true > p_false': False},\n",
              " 'gpt2-xl: Steve Jobs is the founder of': {'p_true': array([0.0001261], dtype=float16),\n",
              "  'p_false': array([2.4e-07], dtype=float16),\n",
              "  'p_true - p_false': array([0.0001259], dtype=float16),\n",
              "  'p_true > p_false': array([ True])}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qmoE3w8FepUZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}